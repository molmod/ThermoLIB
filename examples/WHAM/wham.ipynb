{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thermolib.thermodynamics.fep import BaseFreeEnergyProfile, plot_feps\n",
    "from thermolib.thermodynamics.histogram import Histogram1D,plot_histograms\n",
    "from thermolib.tools import read_wham_input, blav\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as pp, time\n",
    "\n",
    "from molmod.units import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WHAM with error estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison with external WHAM script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define parabolic bias potentials and read corresponding trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature set at 573.000000\n",
      "Added bias Parabola1D (U1): K=500 kjmol  q0=-8.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U1.dat\n",
      "Added bias Parabola1D (U2): K=1000 kjmol  q0=-6.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U2.dat\n",
      "Added bias Parabola1D (U3): K=1000 kjmol  q0=-4.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U3.dat\n",
      "Added bias Parabola1D (U4): K=1000 kjmol  q0=-2.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U4.dat\n",
      "Added bias Parabola1D (U5): K=1000 kjmol  q0=0.000e+00 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U5.dat\n",
      "Added bias Parabola1D (U6): K=1000 kjmol  q0=2.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U6.dat\n",
      "Added bias Parabola1D (U7): K=1000 kjmol  q0=4.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U7.dat\n",
      "Added bias Parabola1D (U8): K=1000 kjmol  q0=6.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U8.dat\n",
      "Added bias Parabola1D (U9): K=500 kjmol  q0=8.000e-01 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U9.dat\n",
      "Added bias Parabola1D (U10): K=500 kjmol  q0=1.000e+00 au\n",
      "Read corresponding trajectory data from /home/louis/hpc/data/shared/massimo/for_Louis/colvar_U10.dat\n"
     ]
    }
   ],
   "source": [
    "#read temperature and bias potentials from wham_input.txt file\n",
    "fn = '/home/louis/hpc/data/shared/massimo/for_Louis/wham_input.txt'\n",
    "temp, biasses, trajectories = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au', stride=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do WHAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper routine defined in the cell below times the execution time of the WHAM1D routine implemented in either pure Python or using Cython. This is to have an idea of the speed up of Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCF Converged!\n",
      "---------------------------------------------------------------------\n",
      "TIMING SUMMARY\n",
      "  initializing: 00h 00m 00.000s\n",
      "  histograms  : 00h 00m 00.003s\n",
      "  bias poten. : 00h 00m 00.318s\n",
      "  solve scf   : 00h 00m 44.830s\n",
      "  error est.  : 00h 00m 00.000s\n",
      "  TOTAL       : 00h 00m 45.151s\n",
      "---------------------------------------------------------------------\n",
      "SCF Converged!\n",
      "---------------------------------------------------------------------\n",
      "TIMING SUMMARY\n",
      "  initializing: 00h 00m 00.001s\n",
      "  histograms  : 00h 00m 00.003s\n",
      "  bias poten. : 00h 00m 00.353s\n",
      "  solve scf   : 00h 00m 00.621s\n",
      "  error est.  : 00h 00m 00.000s\n",
      "  TOTAL       : 00h 00m 00.978s\n",
      "---------------------------------------------------------------------\n",
      "SCF Converged!\n",
      "---------------------------------------------------------------------\n",
      "TIMING SUMMARY\n",
      "  initializing: 00h 00m 00.000s\n",
      "  histograms  : 00h 00m 00.005s\n",
      "  bias poten. : 00h 00m 00.370s\n",
      "  solve scf   : 00h 00m 41.181s\n",
      "  error est.  : 00h 00m 00.103s\n",
      "  TOTAL       : 00h 00m 41.660s\n",
      "---------------------------------------------------------------------\n",
      "SCF Converged!\n",
      "---------------------------------------------------------------------\n",
      "TIMING SUMMARY\n",
      "  initializing: 00h 00m 00.000s\n",
      "  histograms  : 00h 00m 00.005s\n",
      "  bias poten. : 00h 00m 00.340s\n",
      "  solve scf   : 00h 00m 00.558s\n",
      "  error est.  : 00h 00m 00.080s\n",
      "  TOTAL       : 00h 00m 00.984s\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bins = np.arange(-1.2, 1.42, 0.002)\n",
    "hist_ne   = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate=None, Nscf=10000, convergence=1e-9, verbosity='low')\n",
    "hist_ne_c = Histogram1D.from_wham_c(bins, trajectories, biasses, temp, error_estimate=None, Nscf=10000, convergence=1e-9, verbosity='low')\n",
    "#hist_p    = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate='mle_p', Nscf=10000, convergence=1e-9, verbose='low')\n",
    "#hist_p_c  = Histogram1D.from_wham_c(bins, trajectories, biasses, temp, error_estimate='mle_p', Nscf=10000, convergence=1e-9, verbose='low')\n",
    "hist_f    = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate='mle_f', Nscf=10000, convergence=1e-9, verbosity='low')\n",
    "hist_f_c  = Histogram1D.from_wham_c(bins, trajectories, biasses, temp, error_estimate='mle_f', Nscf=10000, convergence=1e-9, verbosity='low')\n",
    "#hist_p = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate='mle_p', Nscf=1000, convergence=1e-6, verbose=False)\n",
    "#hist_f = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate='mle_f', Nscf=1000, convergence=1e-6, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, Cython gives a speed up factor of around 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fep_f = BaseFreeEnergyProfile.from_histogram(hist_p, temp=temp)\n",
    "fep_f.set_ref(ref='min')\n",
    "fep_f_c = BaseFreeEnergyProfile.from_histogram(hist_f, temp=temp)\n",
    "fep_f_c.set_ref(ref='min')\n",
    "plot_feps('feps_compare_cython.png', [fep_f,fep_f_c], labels=['Python', 'Cython'], colors=['k','b'], linestyles=['--','-'], temp=temp, flims=[0.0, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct and plot free energy profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fep_p = BaseFreeEnergyProfile.from_histogram(hist_p, temp=temp)\n",
    "fep_p.set_ref(ref='min')\n",
    "fep_f = BaseFreeEnergyProfile.from_histogram(hist_f, temp=temp)\n",
    "fep_f.set_ref(ref='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with free energy profile generated by external wham script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fep_ext = BaseFreeEnergyProfile.from_txt('/home/louis/hpc/data/shared/massimo/for_Louis/fes.dat', temp, cvcol=0, fcol=1, cv_input_unit='au', f_input_unit='kjmol')\n",
    "fep_ext.set_ref(ref='min')\n",
    "#fep_ext.plot('fep_US_external.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feps('fep_US_compare_error.png', [fep_p,fep_f,fep_ext], labels=['ThermoLIB (mle-p)', 'ThermoLIB (mle-f)', 'WHAMscript'], colors=[None,None,'0.3'], temp=temp, flims=[0.0, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison single histogram method for 'unbiases' simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will apply the WHAM reconstruction to a single unbiased simulation and compare the free energy profile and its error with the direct construction of the single histogram implemented in ThermoLIB. To apply the WHAM reconstructio to a single simulation, we are going to split the simulation into 10 subsequent trajectories and apply WHAM with zero-values biasses.\n",
    "\n",
    "However, for the input data we will use the simulation trajectory of a single umbrella from the umbrella simulation above. Even though this simulation was biases, it doesn't matter for the purpose at hand, i.e. we will suppose it is not biased at all but represents the equilibrium simulation of some system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_single = '/home/louis/hpc/data/shared/massimo/for_Louis/colvar_U8.dat'\n",
    "traj_single = np.loadtxt(fn_single)[:,1]\n",
    "bins = np.arange(0.4, 1.11, 0.01)\n",
    "temp = 573*kelvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct histogram construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_single = Histogram1D.from_single_trajectory(traj_single, bins, error_estimate='mle_f')\n",
    "fep_single = BaseFreeEnergyProfile.from_histogram(hist_single, temp=temp)\n",
    "fep_single.set_ref(ref='min')\n",
    "fep_single.plot('fep_single_direct.png', flim=35*kjmol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAM 'reconstruction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 'trajectories' and 'biases' of pseudo umbrellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nblocks = 5\n",
    "trajectories = [None,]*nblocks\n",
    "biasses = [None,]*nblocks\n",
    "len_block = len(traj_single)//nblocks\n",
    "for iblock in range(nblocks):\n",
    "    block = traj_single[iblock*len_block:(iblock+1)*len_block]\n",
    "    trajectories[iblock] = block\n",
    "    biasses[iblock] = lambda q: np.zeros(len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_wham = Histogram1D.from_wham(bins, trajectories, biasses, temp, error_estimate='mle_f', Nscf=1000, convergence=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fep_wham = BaseFreeEnergyProfile.from_histogram(hist_wham, temp=temp)\n",
    "fep_wham.set_ref(ref='min')\n",
    "#fep_wham.plot('fep_single_wham.png', flim=35*kjmol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feps('fep_single_compare_error.png', [fep_wham,fep_single], labels=['WHAM','single'], flims=[0,35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact correlation on WHAM reconstuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/home/louis/hpc/data/shared/massimo/for_Louis/wham_input.txt'\n",
    "bins = np.arange(-1.2, 1.42, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, biasses, trajectories_1 = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au', stride=1)\n",
    "hist_1 = Histogram1D.from_wham(bins, trajectories_1, biasses, temp, error_estimate='mle_f', Nscf=1000, convergence=1e-6)\n",
    "fep_1 = BaseFreeEnergyProfile.from_histogram(hist_1, temp=temp)\n",
    "fep_1.set_ref(ref='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, biasses, trajectories_100 = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au', stride=100)\n",
    "hist_100 = Histogram1D.from_wham(bins, trajectories_100, biasses, temp, error_estimate='mle_p', Nscf=1000, convergence=1e-6)\n",
    "fep_100 = BaseFreeEnergyProfile.from_histogram(hist_100, temp=temp)\n",
    "fep_100.set_ref(ref='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feps('fep_US_correlation.png', [fep_100,fep_1], labels=['stride=100', 'stride=1'], flims=[0,140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try to estimate the correlation time from block averaging of the collective variable itself during the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, error, corrtime = blav(trajectories[1][::1], blocksizes=np.arange(1,5000,1), fn_plot='trajectory_U2blav.png', plot_ac=True, ac_range=np.arange(0,5000,1), acft_plot_range=[0,0.1])\n",
    "print('Correlation time = %.3f timesteps' %corrtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the WHAM profile 10 times, each time with trajectories at 1 in 100 sub sampling but shifted over 10 steps with respect to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/home/louis/hpc/data/shared/massimo/for_Louis/wham_input.txt'\n",
    "bins = np.arange(-1.2, 1.42, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = []\n",
    "for start in np.arange(0,100,10):\n",
    "    print(\"Constructing WHAM FEP for start=%i\" %start)\n",
    "    temp, biasses, trajectories = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au', start=start, stride=100)\n",
    "    hist = Histogram1D.from_wham(bins, trajectories, biasses, temp)\n",
    "    histograms.append(hist)\n",
    "hist = Histogram1D.from_average(histograms, error_estimate='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms('histograms_US_correlation_2.png', [hist]+histograms, labels=['Average']+['start=%i' %i for i in np.arange(0,100,10)], temp=temp, flims=[0,140],\n",
    "               colors    =['k']+[None,]*10,\n",
    "               linewidths=[2  ]+[1   ,]*10,\n",
    "               linestyles=['-']+['--',]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now devide the original data in 5 subsequent blocks and compute the WHAM FEP for each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nblocks = 5\n",
    "blocksize = 10000\n",
    "histograms = []\n",
    "labels = []\n",
    "for iblock in range(nblocks):\n",
    "    start, end = iblock*blocksize, (iblock+1)*blocksize\n",
    "    print(\"Constructing histogram for block [%i,%i]\" %(start,end))\n",
    "    temp, biasses, trajectories = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au', start=start, end=end, stride=1)\n",
    "    hist = Histogram1D.from_wham(bins, trajectories, biasses, temp)\n",
    "    histograms.append(hist)\n",
    "    labels.append(\"Block[%i,%i]\"%(start,end))\n",
    "hist_blockavg = Histogram1D.from_average(histograms, error_estimate='std', nsigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms('histogram_US_correaltion_3.png', [hist_blockavg]+histograms, labels=['Average']+labels, temp=temp, flims=[0,140], \n",
    "                colors    =['k']+[None,]*nblocks, \n",
    "                linestyles=['-']+['--',]*nblocks, \n",
    "                linewidths=[4  ]+[1   ,]*nblocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feps = []\n",
    "Nboot = 10\n",
    "histograms = []\n",
    "temp, biasses, trajectories = read_wham_input(fn, path_template_colvar_fns='colvar_%s.dat', kappa_unit='kjmol', q0_unit='au')\n",
    "for iboot in range(Nboot):\n",
    "    new_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        Nsteps = len(traj)\n",
    "        indices = np.random.randint(0, high=Nsteps, size=Nsteps)\n",
    "        new_trajectories.append(traj[indices]) \n",
    "    histogram = Histogram1D.from_wham(bins, new_trajectories, biasses, temp, error_estimate='mle_f')\n",
    "    histograms.append(histogram.copy())\n",
    "    feps.append(BaseFreeEnergyProfile.from_histogram(histogram,temp))\n",
    "hist = Histogram1D.from_average(histograms, error_estimate='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms('histograms_US_bootstrapping.png', [hist]+histograms, labels=['Average']+['%i' %i for i in np.arange(0,10,1)], temp=temp, flims=[0,140],\n",
    "               colors    =['k']+[None,]*10,\n",
    "               linewidths=[2  ]+[1   ,]*10,\n",
    "               linestyles=['-']+['--',]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6035c5c19054a81448179272e55352f2864f929f76fb7fd7f7119786402a08c9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
